{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Callable\n",
    "import polars as pl\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataset for the Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParquetDataset(Dataset):\n",
    "    def __init__(self, file_paths: List[str], logging=True):\n",
    "        self.file_paths = file_paths\n",
    "        self.file_row_counts: List[int] = [pq.ParquetFile(f).metadata.num_rows for f in file_paths]\n",
    "        self.cum_row_counts: List[int] = [sum(self.file_row_counts[:i]) for i in range(len(self.file_row_counts))]\n",
    "\n",
    "        self.pq_df: Optional[pl.DataFrame] = None\n",
    "        self.pq_df_idx: Optional[int] = None \n",
    "\n",
    "        if logging:\n",
    "            print(\"Loaded files with rows:\")\n",
    "            for i, file in enumerate(file_paths):\n",
    "                print(f\"\\t{self.file_row_counts[i]} : {file}\")\n",
    "\n",
    "            print(f\"{len(self)} total samples.\" )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return sum(self.file_row_counts)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    def get_single_row_with_batching(self, row_idx: int, file_idx: int) -> pl.DataFrame:\n",
    "        if file_idx != self.pq_df_idx:\n",
    "            self.load_pq_file(idx=file_idx)\n",
    "\n",
    "        return self.pq_df.row(row_idx)\n",
    "\n",
    "    def get_single_row(self, idx: int) -> pl.DataFrame:\n",
    "        # Identify which file and which row within that file corresponds to idx\n",
    "        file_idx: int = self.calculate_file_index_from_global_index(idx)\n",
    "        row_idx: int = idx - self.cum_row_counts[file_idx]\n",
    "        \n",
    "        # Load data for the required row\n",
    "        return self.get_row_from_pq_file(file=self.file_paths[file_idx], row_idx=row_idx)\n",
    "\n",
    "    def calculate_file_index_from_global_index(self, idx: int) -> int:\n",
    "        for i, cum_row_count in enumerate(self.cum_row_counts):\n",
    "            if cum_row_count <= idx:\n",
    "                return i\n",
    "\n",
    "        raise ValueError(f\"Index {idx} too large for dataset with {len(self)} samples\")\n",
    "                \n",
    "    @staticmethod\n",
    "    def get_row_from_pq_file(file: str, row_idx: int):\n",
    "        return pl.read_parquet(file, row_index_offset=row_idx, n_rows=1, use_pyarrow=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_df_row_to_tensor(row: pl.DataFrame) -> torch.Tensor:\n",
    "        # TODO\n",
    "        return row\n",
    "    \n",
    "    def load_pq_file(self, idx: int) -> None:\n",
    "        self.pq_df_idx = idx\n",
    "        self.pq_df = pl.read_parquet(self.file_paths[idx])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T00:45:13.285928Z",
     "iopub.status.busy": "2024-11-11T00:45:13.285505Z",
     "iopub.status.idle": "2024-11-11T00:46:47.707723Z",
     "shell.execute_reply": "2024-11-11T00:46:47.705581Z",
     "shell.execute_reply.started": "2024-11-11T00:45:13.285885Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded files with rows:\n",
      "\t1944210 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0/part-0.parquet\n",
      "\t2804247 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=1/part-0.parquet\n",
      "\t3036873 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=2/part-0.parquet\n",
      "\t4016784 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=3/part-0.parquet\n",
      "\t5022952 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=4/part-0.parquet\n",
      "\t5348200 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=5/part-0.parquet\n",
      "\t6203912 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=6/part-0.parquet\n",
      "\t6335560 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=7/part-0.parquet\n",
      "\t6140024 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=8/part-0.parquet\n",
      "\t6274576 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=9/part-0.parquet\n",
      "47127338 total samples.\n"
     ]
    }
   ],
   "source": [
    "def make_train_parquet_path(i: int) -> str:\n",
    "    return f\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id={i}/part-0.parquet\"\n",
    "# Setup the file indices to use.\n",
    "K_MAX_TRAIN_FILES: int = 10\n",
    "K_TRAIN_FILES: List[str] = [make_train_parquet_path(i) for i in range(K_MAX_TRAIN_FILES)]\n",
    "K_TEST_FILES: List[str] = [\"/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet/date_id=0/part-0.parquet\"]\n",
    "\n",
    "# K_TRAIN_FILE_INDICES: List[int] = [8]\n",
    "\n",
    "train_dataset = ParquetDataset(file_paths=K_TRAIN_FILES, logging=True)\n",
    "# test_dataset = ParquetDataset(file_paths=K_TEST_FILES, logging=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:'iterate_samples_by_row' args:[(), {'step_size': 1000000}] took: 1.1238 sec\n",
      "func:'iterate_samples_by_row' args:[(), {'step_size': 100000}] took: 12.2215 sec\n",
      "func:'iterate_samples_by_row' args:[(), {'step_size': 10000}] took: 99.2402 sec\n",
      "func:'iterate_samples_batched' args:[(), {'step_size': 1000}] took: 11.4278 sec\n",
      "func:'iterate_samples_batched' args:[(), {'step_size': 100}] took: 16.0547 sec\n",
      "func:'iterate_samples_batched' args:[(), {'step_size': 10}] took: 44.7318 sec\n"
     ]
    }
   ],
   "source": [
    "from functools import wraps\n",
    "from time import time\n",
    "import random\n",
    "\n",
    "def timing(f: Callable, *args, **kwargs) -> Callable:\n",
    "    @wraps(f)\n",
    "    def wrap(*args, **kwargs):\n",
    "        ts = time()\n",
    "        result = f(*args, **kwargs)\n",
    "        te = time()\n",
    "        print('func:%r args:[%r, %r] took: %2.4f sec' % \\\n",
    "          (f.__name__, args, kwargs, te-ts))\n",
    "        return result\n",
    "    return wrap\n",
    "\n",
    "@timing\n",
    "def iterate_samples_by_row(step_size: int):\n",
    "    for i in range(0, len(train_dataset), step_size):\n",
    "        train_dataset.get_single_row(i)\n",
    "\n",
    "@timing\n",
    "def iterate_samples_batched(step_size: int):\n",
    "    for file_idx in range(K_MAX_TRAIN_FILES):\n",
    "        for row_idx in range(0, train_dataset.file_row_counts[file_idx], step_size):\n",
    "            train_dataset.get_single_row_with_batching(row_idx=row_idx, file_idx=file_idx)\n",
    "\n",
    "# A simple batching schemem is definitely the way to go. We can iterate orders of magnitude more samples\n",
    "# in the same amount of time it would take to do naive random access.\n",
    "iterate_samples_by_row(step_size=1000000)\n",
    "iterate_samples_by_row(step_size=100000)\n",
    "iterate_samples_by_row(step_size=10000)\n",
    "iterate_samples_batched(step_size=1000)\n",
    "iterate_samples_batched(step_size=100)\n",
    "iterate_samples_batched(step_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T21:17:16.113402Z",
     "iopub.status.busy": "2024-11-10T21:17:16.112741Z",
     "iopub.status.idle": "2024-11-10T21:17:19.029436Z",
     "shell.execute_reply": "2024-11-10T21:17:19.028057Z",
     "shell.execute_reply.started": "2024-11-10T21:17:16.113331Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "# Separate features and responders\n",
    "features = sample_df.filter(regex='^feature_')\n",
    "responders = sample_df.filter(regex='^responder_')\n",
    "# Convert to numpy arrays for TensorFlow\n",
    "X = features.values  # Features for input\n",
    "y = responders.values  # Responders for output\n",
    "X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "y = np.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T21:17:19.034077Z",
     "iopub.status.busy": "2024-11-10T21:17:19.033604Z",
     "iopub.status.idle": "2024-11-10T21:17:19.098752Z",
     "shell.execute_reply": "2024-11-10T21:17:19.097454Z",
     "shell.execute_reply.started": "2024-11-10T21:17:19.034029Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the number of input and output nodes\n",
    "input_dim = X.shape[1]  # Number of features (79)\n",
    "output_dim = y.shape[1]  # Number of responders (9)\n",
    "# Define the model\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(input_dim,)),  # Input layer\n",
    "    layers.Dense(64, activation='relu'),  # Encoder\n",
    "    layers.Dense(32, activation='relu'),  # Bottleneck layer (compression)\n",
    "    layers.Dense(64, activation='relu'),  # Decoder\n",
    "    layers.Dense(output_dim, activation='linear')  # Output layer for responders\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T21:17:19.100583Z",
     "iopub.status.busy": "2024-11-10T21:17:19.100159Z",
     "iopub.status.idle": "2024-11-10T21:17:19.106868Z",
     "shell.execute_reply": "2024-11-10T21:17:19.105404Z",
     "shell.execute_reply.started": "2024-11-10T21:17:19.100541Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "def step_decay(epoch):\n",
    "    initial_lr = 0.01\n",
    "    drop = 0.5\n",
    "    epochs_drop = 5\n",
    "    lr = initial_lr * (drop ** (epoch // epochs_drop))\n",
    "    return lr\n",
    "lr_scheduler = LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T21:17:19.109203Z",
     "iopub.status.busy": "2024-11-10T21:17:19.108678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Define EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',    # Monitor validation loss\n",
    "    patience=10,            # Number of epochs to wait for improvement\n",
    "    min_delta=0.001,       # Minimum change to qualify as an improvement\n",
    "    restore_best_weights=True  # Restore weights from the best epoch\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X, y,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save(\"/kaggle/working/model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "See [Jane Street RMF Demo Submission](https://www.kaggle.com/code/ryanholbrook/jane-street-rmf-demo-submission) for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import kaggle_evaluation.jane_street_inference_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "# Assuming `model` is your trained model\n",
    "# Assuming features required by the model are named 'feature_00', 'feature_01', etc.\n",
    "def predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Make a prediction.\"\"\"\n",
    "    global lags_\n",
    "    if lags is not None:\n",
    "        lags_ = lags\n",
    "    # Extract the features for the model input\n",
    "    feature_columns = [col for col in test.columns if col.startswith(\"feature_\")]\n",
    "    features = test.select(feature_columns).to_numpy()  # Convert to numpy array for model input\n",
    "    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    # Generate predictions using the model\n",
    "    model_predictions = model.predict(features)\n",
    "    responder_6_predictions = model_predictions[:, 6]  # Assuming responder_6 is at index 6\n",
    "    # Create a new Polars DataFrame with row_id and responder_6 predictions\n",
    "    predictions = test.select(\"row_id\").with_columns(\n",
    "        pl.Series(\"responder_6\", responder_6_predictions)\n",
    "    )\n",
    "    # Ensure the output format and length requirements\n",
    "    if isinstance(predictions, pl.DataFrame):\n",
    "        assert predictions.columns == ['row_id', 'responder_6']\n",
    "    elif isinstance(predictions, pd.DataFrame):\n",
    "        assert (predictions.columns == ['row_id', 'responder_6']).all()\n",
    "    else:\n",
    "        raise TypeError('The predict function must return a DataFrame')\n",
    "    \n",
    "    assert len(predictions) == len(test)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inference_server = kaggle_evaluation.jane_street_inference_server.JSInferenceServer(predict)\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "            '/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet',\n",
    "            '/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet',\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9871156,
     "sourceId": 84493,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
