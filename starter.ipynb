{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Callable, Tuple\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "from time import time\n",
    "\n",
    "def timing(f: Callable, *args, **kwargs) -> Callable:\n",
    "    @wraps(f)\n",
    "    def wrap(*args, **kwargs):\n",
    "        ts = time()\n",
    "        result = f(*args, **kwargs)\n",
    "        te = time()\n",
    "        print('func:%r args:[%r, %r] took: %2.4f sec' % \\\n",
    "          (f.__name__, args, kwargs, te-ts))\n",
    "        return result\n",
    "    return wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset for the Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parquet_dataset.parquet_dataset import ParquetDataset\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T00:45:13.285928Z",
     "iopub.status.busy": "2024-11-11T00:45:13.285505Z",
     "iopub.status.idle": "2024-11-11T00:46:47.707723Z",
     "shell.execute_reply": "2024-11-11T00:46:47.705581Z",
     "shell.execute_reply.started": "2024-11-11T00:45:13.285885Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded files with rows:\n",
      "\t1944210 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0/part-0.parquet\n",
      "\t2804247 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=1/part-0.parquet\n",
      "\t3036873 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=2/part-0.parquet\n",
      "\t4016784 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=3/part-0.parquet\n",
      "\t5022952 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=4/part-0.parquet\n",
      "\t5348200 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=5/part-0.parquet\n",
      "\t6203912 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=6/part-0.parquet\n",
      "\t6335560 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=7/part-0.parquet\n",
      "\t6140024 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=8/part-0.parquet\n",
      "\t6274576 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=9/part-0.parquet\n",
      "47127338 total samples.\n"
     ]
    }
   ],
   "source": [
    "def make_train_parquet_path(i: int) -> str:\n",
    "    return f\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id={i}/part-0.parquet\"\n",
    "# Setup the file indices to use.\n",
    "K_MAX_TRAIN_FILES: int = 10\n",
    "K_TRAIN_FILES: List[str] = [make_train_parquet_path(i) for i in range(K_MAX_TRAIN_FILES)]\n",
    "K_TEST_FILES: List[str] = [\"/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet/date_id=0/part-0.parquet\"]\n",
    "\n",
    "train_dataset = ParquetDataset(file_paths=K_TRAIN_FILES, logging=True)\n",
    "# test_dataset = ParquetDataset(file_paths=K_TEST_FILES, logging=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Run timing analysis to check batched approach vs. naive single-row loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:'iterate_samples_batched_row_group' args:[(), {'step_size': 1000}] took: 23.3475 sec\n",
      "func:'iterate_samples_batched_row_group' args:[(), {'step_size': 100}] took: 37.6389 sec\n"
     ]
    }
   ],
   "source": [
    "@timing\n",
    "def iterate_samples_batched_row_group(step_size: int):\n",
    "    for i in range(0, len(train_dataset), step_size):\n",
    "        train_dataset[i]\n",
    "\n",
    "iterate_samples_batched_row_group(step_size=1000)\n",
    "iterate_samples_batched_row_group(step_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a custom sample for randomly ordering the parquet files and rows within the parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from parquet_sampler.parquet_sampler import ParquetBatchedSampler\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Verify that the sampler produces samples in random parition order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 47127338\n",
      "found new file 115\n",
      "found new file 50\n",
      "found new file 25\n",
      "found new file 125\n",
      "1000000 of 47127338\n",
      "found new file 75\n",
      "found new file 63\n",
      "found new file 2\n",
      "found new file 51\n",
      "2000000 of 47127338\n",
      "found new file 154\n",
      "found new file 70\n",
      "found new file 22\n",
      "found new file 155\n",
      "3000000 of 47127338\n",
      "found new file 158\n",
      "found new file 9\n",
      "found new file 74\n",
      "4000000 of 47127338\n",
      "found new file 37\n",
      "found new file 5\n",
      "found new file 111\n",
      "found new file 35\n",
      "5000000 of 47127338\n",
      "found new file 62\n",
      "found new file 20\n",
      "found new file 110\n",
      "found new file 55\n",
      "6000000 of 47127338\n",
      "found new file 86\n",
      "found new file 171\n",
      "found new file 64\n",
      "7000000 of 47127338\n",
      "found new file 141\n",
      "found new file 48\n",
      "found new file 80\n",
      "found new file 131\n",
      "8000000 of 47127338\n",
      "found new file 61\n",
      "found new file 173\n",
      "found new file 0\n",
      "found new file 135\n",
      "9000000 of 47127338\n",
      "found new file 26\n",
      "found new file 18\n",
      "found new file 68\n",
      "found new file 77\n",
      "10000000 of 47127338\n",
      "found new file 124\n",
      "found new file 98\n",
      "found new file 157\n",
      "11000000 of 47127338\n",
      "found new file 39\n",
      "found new file 147\n",
      "found new file 123\n",
      "found new file 97\n",
      "12000000 of 47127338\n",
      "found new file 132\n",
      "found new file 17\n",
      "found new file 122\n",
      "found new file 151\n",
      "13000000 of 47127338\n",
      "found new file 163\n",
      "found new file 84\n",
      "found new file 52\n",
      "14000000 of 47127338\n",
      "found new file 43\n",
      "found new file 102\n",
      "found new file 107\n",
      "found new file 127\n",
      "15000000 of 47127338\n",
      "found new file 72\n",
      "found new file 53\n",
      "found new file 126\n",
      "found new file 66\n",
      "16000000 of 47127338\n",
      "found new file 160\n",
      "found new file 82\n",
      "found new file 114\n",
      "found new file 38\n",
      "17000000 of 47127338\n",
      "found new file 95\n",
      "found new file 149\n",
      "found new file 41\n",
      "18000000 of 47127338\n",
      "found new file 8\n",
      "found new file 143\n",
      "found new file 69\n",
      "found new file 168\n",
      "19000000 of 47127338\n",
      "found new file 28\n",
      "found new file 76\n",
      "found new file 12\n",
      "found new file 30\n",
      "20000000 of 47127338\n",
      "found new file 11\n",
      "found new file 89\n",
      "found new file 40\n",
      "found new file 45\n",
      "21000000 of 47127338\n",
      "found new file 156\n",
      "found new file 138\n",
      "found new file 142\n",
      "22000000 of 47127338\n",
      "found new file 54\n",
      "found new file 16\n",
      "found new file 113\n",
      "found new file 150\n",
      "23000000 of 47127338\n",
      "found new file 137\n",
      "found new file 119\n",
      "found new file 121\n",
      "found new file 94\n",
      "24000000 of 47127338\n",
      "found new file 46\n",
      "found new file 57\n",
      "found new file 44\n",
      "25000000 of 47127338\n",
      "found new file 140\n",
      "found new file 60\n",
      "found new file 23\n",
      "found new file 71\n",
      "26000000 of 47127338\n",
      "found new file 130\n",
      "found new file 153\n",
      "found new file 92\n",
      "found new file 104\n",
      "27000000 of 47127338\n",
      "found new file 27\n",
      "found new file 33\n",
      "found new file 32\n",
      "found new file 29\n",
      "28000000 of 47127338\n",
      "found new file 24\n",
      "found new file 4\n",
      "found new file 108\n",
      "29000000 of 47127338\n",
      "found new file 172\n",
      "found new file 103\n",
      "found new file 67\n",
      "found new file 93\n",
      "30000000 of 47127338\n",
      "found new file 139\n",
      "found new file 164\n",
      "found new file 133\n",
      "found new file 167\n",
      "31000000 of 47127338\n",
      "found new file 120\n",
      "found new file 159\n",
      "found new file 144\n",
      "32000000 of 47127338\n",
      "found new file 109\n",
      "found new file 128\n",
      "found new file 145\n",
      "found new file 47\n",
      "33000000 of 47127338\n",
      "found new file 106\n",
      "found new file 117\n",
      "found new file 129\n",
      "found new file 87\n",
      "34000000 of 47127338\n",
      "found new file 136\n",
      "found new file 81\n",
      "found new file 6\n",
      "found new file 13\n",
      "35000000 of 47127338\n",
      "found new file 14\n",
      "found new file 152\n",
      "found new file 90\n",
      "36000000 of 47127338\n",
      "found new file 170\n",
      "found new file 34\n",
      "found new file 19\n",
      "found new file 100\n",
      "37000000 of 47127338\n",
      "found new file 10\n",
      "found new file 1\n",
      "found new file 83\n",
      "found new file 78\n",
      "38000000 of 47127338\n",
      "found new file 161\n",
      "found new file 91\n",
      "found new file 73\n",
      "39000000 of 47127338\n",
      "found new file 112\n",
      "found new file 99\n",
      "found new file 56\n",
      "found new file 148\n",
      "40000000 of 47127338\n",
      "found new file 174\n",
      "found new file 59\n",
      "found new file 96\n",
      "found new file 165\n",
      "41000000 of 47127338\n",
      "found new file 3\n",
      "found new file 85\n",
      "found new file 65\n",
      "42000000 of 47127338\n",
      "found new file 58\n",
      "found new file 166\n",
      "found new file 162\n",
      "found new file 116\n",
      "43000000 of 47127338\n",
      "found new file 101\n",
      "found new file 105\n",
      "found new file 7\n",
      "found new file 15\n",
      "44000000 of 47127338\n",
      "found new file 21\n",
      "found new file 169\n",
      "found new file 31\n",
      "found new file 36\n",
      "45000000 of 47127338\n",
      "found new file 49\n",
      "found new file 79\n",
      "found new file 146\n",
      "46000000 of 47127338\n",
      "found new file 88\n",
      "found new file 134\n",
      "found new file 118\n",
      "found new file 42\n",
      "47000000 of 47127338\n",
      "Total elements: 47127338. Saw 175, partition ordering: [np.int64(115), np.int64(50), np.int64(25), np.int64(125), np.int64(75), np.int64(63), np.int64(2), np.int64(51), np.int64(154), np.int64(70), np.int64(22), np.int64(155), np.int64(158), np.int64(9), np.int64(74), np.int64(37), np.int64(5), np.int64(111), np.int64(35), np.int64(62), np.int64(20), np.int64(110), np.int64(55), np.int64(86), np.int64(171), np.int64(64), np.int64(141), np.int64(48), np.int64(80), np.int64(131), np.int64(61), np.int64(173), np.int64(0), np.int64(135), np.int64(26), np.int64(18), np.int64(68), np.int64(77), np.int64(124), np.int64(98), np.int64(157), np.int64(39), np.int64(147), np.int64(123), np.int64(97), np.int64(132), np.int64(17), np.int64(122), np.int64(151), np.int64(163), np.int64(84), np.int64(52), np.int64(43), np.int64(102), np.int64(107), np.int64(127), np.int64(72), np.int64(53), np.int64(126), np.int64(66), np.int64(160), np.int64(82), np.int64(114), np.int64(38), np.int64(95), np.int64(149), np.int64(41), np.int64(8), np.int64(143), np.int64(69), np.int64(168), np.int64(28), np.int64(76), np.int64(12), np.int64(30), np.int64(11), np.int64(89), np.int64(40), np.int64(45), np.int64(156), np.int64(138), np.int64(142), np.int64(54), np.int64(16), np.int64(113), np.int64(150), np.int64(137), np.int64(119), np.int64(121), np.int64(94), np.int64(46), np.int64(57), np.int64(44), np.int64(140), np.int64(60), np.int64(23), np.int64(71), np.int64(130), np.int64(153), np.int64(92), np.int64(104), np.int64(27), np.int64(33), np.int64(32), np.int64(29), np.int64(24), np.int64(4), np.int64(108), np.int64(172), np.int64(103), np.int64(67), np.int64(93), np.int64(139), np.int64(164), np.int64(133), np.int64(167), np.int64(120), np.int64(159), np.int64(144), np.int64(109), np.int64(128), np.int64(145), np.int64(47), np.int64(106), np.int64(117), np.int64(129), np.int64(87), np.int64(136), np.int64(81), np.int64(6), np.int64(13), np.int64(14), np.int64(152), np.int64(90), np.int64(170), np.int64(34), np.int64(19), np.int64(100), np.int64(10), np.int64(1), np.int64(83), np.int64(78), np.int64(161), np.int64(91), np.int64(73), np.int64(112), np.int64(99), np.int64(56), np.int64(148), np.int64(174), np.int64(59), np.int64(96), np.int64(165), np.int64(3), np.int64(85), np.int64(65), np.int64(58), np.int64(166), np.int64(162), np.int64(116), np.int64(101), np.int64(105), np.int64(7), np.int64(15), np.int64(21), np.int64(169), np.int64(31), np.int64(36), np.int64(49), np.int64(79), np.int64(146), np.int64(88), np.int64(134), np.int64(118), np.int64(42)]\n"
     ]
    }
   ],
   "source": [
    "sampler = ParquetBatchedSampler(data_source=train_dataset)\n",
    "partition_ordering: List[Tuple[int,int]] = []\n",
    "num_sampled_per_partition: List[int] = []\n",
    "curr_partition: Optional[Tuple[int, int]] = None\n",
    "\n",
    "for i, idx in enumerate(sampler):\n",
    "\n",
    "    if (i % 1000000 == 0):\n",
    "        print(f\"{i} of {len(sampler)}\\r\")\n",
    "\n",
    "    new_idx = train_dataset.calculate_index_from_cumulative_counts(idx, train_dataset.cum_total_counts_np)\n",
    "    if curr_partition != new_idx:\n",
    "        # update the partition index assuming that once we see a file index and then leave it\n",
    "        # we'll never see it again. Throw an error if we do see it again.\n",
    "        if new_idx in partition_ordering:\n",
    "            raise ValueError(f\"File index {new_idx} seen out of order! Already seen ordering: {partition_ordering}\")\n",
    "        print(f\"found new file {new_idx}\")\n",
    "        curr_partition = new_idx\n",
    "        partition_ordering.append(curr_partition)\n",
    "        if len(num_sampled_per_partition) > 0:\n",
    "            num_sampled_per_partition.append(i - num_sampled_per_partition[-1])\n",
    "        else:\n",
    "            num_sampled_per_partition.append(i)\n",
    "\n",
    "print(f\"Total elements: {len(sampler)}. Saw {len(partition_ordering)}, partition ordering: {partition_ordering}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Benchmarking how long it takes to serve samples from the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apparently there's an issue with \n",
    "@timing\n",
    "def iterate_dataloader(num_workers=0, batch_size=1, prefetch_factor=1):\n",
    "    dl = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        # prefetch_factor=prefetch_factor,\n",
    "        sampler=ParquetBatchedSampler(data_source=train_dataset)\n",
    "    )\n",
    "    print(\"Created dataloader.\")\n",
    "    for i, batch in enumerate(dl):\n",
    "        if i % 1000 == 0:\n",
    "            print(i, len(dl))\n",
    "\n",
    "iterate_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T21:17:16.113402Z",
     "iopub.status.busy": "2024-11-10T21:17:16.112741Z",
     "iopub.status.idle": "2024-11-10T21:17:19.029436Z",
     "shell.execute_reply": "2024-11-10T21:17:19.028057Z",
     "shell.execute_reply.started": "2024-11-10T21:17:16.113331Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "# Separate features and responders\n",
    "features = sample_df.filter(regex='^feature_')\n",
    "responders = sample_df.filter(regex='^responder_')\n",
    "# Convert to numpy arrays for TensorFlow\n",
    "X = features.values  # Features for input\n",
    "y = responders.values  # Responders for output\n",
    "X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "y = np.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T21:17:19.034077Z",
     "iopub.status.busy": "2024-11-10T21:17:19.033604Z",
     "iopub.status.idle": "2024-11-10T21:17:19.098752Z",
     "shell.execute_reply": "2024-11-10T21:17:19.097454Z",
     "shell.execute_reply.started": "2024-11-10T21:17:19.034029Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the number of input and output nodes\n",
    "input_dim = X.shape[1]  # Number of features (79)\n",
    "output_dim = y.shape[1]  # Number of responders (9)\n",
    "# Define the model\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(input_dim,)),  # Input layer\n",
    "    layers.Dense(64, activation='relu'),  # Encoder\n",
    "    layers.Dense(32, activation='relu'),  # Bottleneck layer (compression)\n",
    "    layers.Dense(64, activation='relu'),  # Decoder\n",
    "    layers.Dense(output_dim, activation='linear')  # Output layer for responders\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T21:17:19.100583Z",
     "iopub.status.busy": "2024-11-10T21:17:19.100159Z",
     "iopub.status.idle": "2024-11-10T21:17:19.106868Z",
     "shell.execute_reply": "2024-11-10T21:17:19.105404Z",
     "shell.execute_reply.started": "2024-11-10T21:17:19.100541Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "def step_decay(epoch):\n",
    "    initial_lr = 0.01\n",
    "    drop = 0.5\n",
    "    epochs_drop = 5\n",
    "    lr = initial_lr * (drop ** (epoch // epochs_drop))\n",
    "    return lr\n",
    "lr_scheduler = LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T21:17:19.109203Z",
     "iopub.status.busy": "2024-11-10T21:17:19.108678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Define EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',    # Monitor validation loss\n",
    "    patience=10,            # Number of epochs to wait for improvement\n",
    "    min_delta=0.001,       # Minimum change to qualify as an improvement\n",
    "    restore_best_weights=True  # Restore weights from the best epoch\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X, y,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save(\"/kaggle/working/model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "See [Jane Street RMF Demo Submission](https://www.kaggle.com/code/ryanholbrook/jane-street-rmf-demo-submission) for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import kaggle_evaluation.jane_street_inference_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "# Assuming `model` is your trained model\n",
    "# Assuming features required by the model are named 'feature_00', 'feature_01', etc.\n",
    "def predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Make a prediction.\"\"\"\n",
    "    global lags_\n",
    "    if lags is not None:\n",
    "        lags_ = lags\n",
    "    # Extract the features for the model input\n",
    "    feature_columns = [col for col in test.columns if col.startswith(\"feature_\")]\n",
    "    features = test.select(feature_columns).to_numpy()  # Convert to numpy array for model input\n",
    "    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    # Generate predictions using the model\n",
    "    model_predictions = model.predict(features)\n",
    "    responder_6_predictions = model_predictions[:, 6]  # Assuming responder_6 is at index 6\n",
    "    # Create a new Polars DataFrame with row_id and responder_6 predictions\n",
    "    predictions = test.select(\"row_id\").with_columns(\n",
    "        pl.Series(\"responder_6\", responder_6_predictions)\n",
    "    )\n",
    "    # Ensure the output format and length requirements\n",
    "    if isinstance(predictions, pl.DataFrame):\n",
    "        assert predictions.columns == ['row_id', 'responder_6']\n",
    "    elif isinstance(predictions, pd.DataFrame):\n",
    "        assert (predictions.columns == ['row_id', 'responder_6']).all()\n",
    "    else:\n",
    "        raise TypeError('The predict function must return a DataFrame')\n",
    "    \n",
    "    assert len(predictions) == len(test)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inference_server = kaggle_evaluation.jane_street_inference_server.JSInferenceServer(predict)\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "            '/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet',\n",
    "            '/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet',\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9871156,
     "sourceId": 84493,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
