{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset for the Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from parquet_dataset.parquet_dataset import ParquetDataset\n",
    "from parquet_dataset.parquet_dataset import DatetimeParquetDataset\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T00:45:13.285928Z",
     "iopub.status.busy": "2024-11-11T00:45:13.285505Z",
     "iopub.status.idle": "2024-11-11T00:46:47.707723Z",
     "shell.execute_reply": "2024-11-11T00:46:47.705581Z",
     "shell.execute_reply.started": "2024-11-11T00:45:13.285885Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded files with rows:\n",
      "\t1944210 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0/part-0.parquet\n",
      "\t2804247 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=1/part-0.parquet\n",
      "\t3036873 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=2/part-0.parquet\n",
      "\t4016784 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=3/part-0.parquet\n",
      "\t5022952 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=4/part-0.parquet\n",
      "\t5348200 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=5/part-0.parquet\n",
      "\t6203912 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=6/part-0.parquet\n",
      "\t6335560 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=7/part-0.parquet\n",
      "\t6140024 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=8/part-0.parquet\n",
      "\t6274576 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=9/part-0.parquet\n",
      "47127338 total rows.\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def make_train_parquet_path(i: int) -> str:\n",
    "    return f\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id={i}/part-0.parquet\"\n",
    "# Setup the file indices to use.\n",
    "K_MAX_TRAIN_FILES: int = 10\n",
    "K_TRAIN_FILES: List[str] = [make_train_parquet_path(i) for i in range(K_MAX_TRAIN_FILES)]\n",
    "K_TEST_FILES: List[str] = [\"/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet/date_id=0/part-0.parquet\"]\n",
    "\n",
    "train_dataset = ParquetDataset(file_paths=K_TRAIN_FILES, logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_train_dataset = DatetimeParquetDataset(\n",
    "    file_paths=K_TRAIN_FILES[0:2], \n",
    "    logging=False,\n",
    "    time_context_length=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288660\n",
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [8, 92] at entry 0 and [12, 92] at entry 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mdt_train_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# print(dt_train_dataset.date_time_segments[i])\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# print(dt_train_dataset[i].size())\u001b[39;00m\n",
      "File \u001b[0;32m~/kaggle-jane-street-forecasting/parquet_dataset/parquet_dataset.py:253\u001b[0m, in \u001b[0;36mDatetimeParquetDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_dt_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_context_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [8, 92] at entry 0 and [12, 92] at entry 2"
     ]
    }
   ],
   "source": [
    "print(len(dt_train_dataset))\n",
    "for i in range(len(dt_train_dataset)):\n",
    "    if i % 10000 == 0:\n",
    "        print(i)\n",
    "    dt_train_dataset[i]\n",
    "    # print(dt_train_dataset.date_time_segments[i])\n",
    "    # print(dt_train_dataset[i].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample to populate the row group cache\n",
    "idx = 0\n",
    "symbols = set()\n",
    "dates = set()\n",
    "while idx < len(train_dataset):\n",
    "    train_dataset[idx]\n",
    "    idx += len(train_dataset.pq_cache.df)\n",
    "    print(train_dataset.pq_cache.df.head())\n",
    "    print(train_dataset.pq_cache.df.tail())\n",
    "    symbols.update(set(train_dataset.pq_cache.df.get_column(\"symbol_id\")))\n",
    "    dates.update(set(train_dataset.pq_cache.df.get_column(\"date_id\")))\n",
    "\n",
    "print(symbols)\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a custom sample for randomly ordering the parquet files and rows within the parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from parquet_sampler.parquet_sampler import ParquetBatchedSampler\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=64,\n",
    "    num_workers=12,\n",
    "    prefetch_factor=2,\n",
    "    sampler=ParquetBatchedSampler(data_source=train_dataset)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T21:17:16.113402Z",
     "iopub.status.busy": "2024-11-10T21:17:16.112741Z",
     "iopub.status.idle": "2024-11-10T21:17:19.029436Z",
     "shell.execute_reply": "2024-11-10T21:17:19.028057Z",
     "shell.execute_reply.started": "2024-11-10T21:17:16.113331Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "# Separate features and responders\n",
    "features = sample_df.filter(regex='^feature_')\n",
    "responders = sample_df.filter(regex='^responder_')\n",
    "# Convert to numpy arrays for TensorFlow\n",
    "X = features.values  # Features for input\n",
    "y = responders.values  # Responders for output\n",
    "X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "y = np.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T21:17:19.034077Z",
     "iopub.status.busy": "2024-11-10T21:17:19.033604Z",
     "iopub.status.idle": "2024-11-10T21:17:19.098752Z",
     "shell.execute_reply": "2024-11-10T21:17:19.097454Z",
     "shell.execute_reply.started": "2024-11-10T21:17:19.034029Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the number of input and output nodes\n",
    "input_dim = X.shape[1]  # Number of features (79)\n",
    "output_dim = y.shape[1]  # Number of responders (9)\n",
    "# Define the model\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(input_dim,)),  # Input layer\n",
    "    layers.Dense(64, activation='relu'),  # Encoder\n",
    "    layers.Dense(32, activation='relu'),  # Bottleneck layer (compression)\n",
    "    layers.Dense(64, activation='relu'),  # Decoder\n",
    "    layers.Dense(output_dim, activation='linear')  # Output layer for responders\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T21:17:19.100583Z",
     "iopub.status.busy": "2024-11-10T21:17:19.100159Z",
     "iopub.status.idle": "2024-11-10T21:17:19.106868Z",
     "shell.execute_reply": "2024-11-10T21:17:19.105404Z",
     "shell.execute_reply.started": "2024-11-10T21:17:19.100541Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "def step_decay(epoch):\n",
    "    initial_lr = 0.01\n",
    "    drop = 0.5\n",
    "    epochs_drop = 5\n",
    "    lr = initial_lr * (drop ** (epoch // epochs_drop))\n",
    "    return lr\n",
    "lr_scheduler = LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T21:17:19.109203Z",
     "iopub.status.busy": "2024-11-10T21:17:19.108678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Define EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',    # Monitor validation loss\n",
    "    patience=10,            # Number of epochs to wait for improvement\n",
    "    min_delta=0.001,       # Minimum change to qualify as an improvement\n",
    "    restore_best_weights=True  # Restore weights from the best epoch\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X, y,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save(\"/kaggle/working/model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "See [Jane Street RMF Demo Submission](https://www.kaggle.com/code/ryanholbrook/jane-street-rmf-demo-submission) for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import kaggle_evaluation.jane_street_inference_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "# Assuming `model` is your trained model\n",
    "# Assuming features required by the model are named 'feature_00', 'feature_01', etc.\n",
    "def predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Make a prediction.\"\"\"\n",
    "    global lags_\n",
    "    if lags is not None:\n",
    "        lags_ = lags\n",
    "    # Extract the features for the model input\n",
    "    feature_columns = [col for col in test.columns if col.startswith(\"feature_\")]\n",
    "    features = test.select(feature_columns).to_numpy()  # Convert to numpy array for model input\n",
    "    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    # Generate predictions using the model\n",
    "    model_predictions = model.predict(features)\n",
    "    responder_6_predictions = model_predictions[:, 6]  # Assuming responder_6 is at index 6\n",
    "    # Create a new Polars DataFrame with row_id and responder_6 predictions\n",
    "    predictions = test.select(\"row_id\").with_columns(\n",
    "        pl.Series(\"responder_6\", responder_6_predictions)\n",
    "    )\n",
    "    # Ensure the output format and length requirements\n",
    "    if isinstance(predictions, pl.DataFrame):\n",
    "        assert predictions.columns == ['row_id', 'responder_6']\n",
    "    elif isinstance(predictions, pd.DataFrame):\n",
    "        assert (predictions.columns == ['row_id', 'responder_6']).all()\n",
    "    else:\n",
    "        raise TypeError('The predict function must return a DataFrame')\n",
    "    \n",
    "    assert len(predictions) == len(test)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inference_server = kaggle_evaluation.jane_street_inference_server.JSInferenceServer(predict)\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "            '/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet',\n",
    "            '/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet',\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9871156,
     "sourceId": 84493,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
