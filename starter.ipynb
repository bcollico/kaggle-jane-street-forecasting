{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Callable, Iterator\n",
    "import polars as pl\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "from torch.utils.data import Dataset, Sampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "from time import time\n",
    "\n",
    "def timing(f: Callable, *args, **kwargs) -> Callable:\n",
    "    @wraps(f)\n",
    "    def wrap(*args, **kwargs):\n",
    "        ts = time()\n",
    "        result = f(*args, **kwargs)\n",
    "        te = time()\n",
    "        print('func:%r args:[%r, %r] took: %2.4f sec' % \\\n",
    "          (f.__name__, args, kwargs, te-ts))\n",
    "        return result\n",
    "    return wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset for the Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParquetDataset(Dataset):\n",
    "    def __init__(self, file_paths: List[str], logging=True):\n",
    "        self.file_paths = file_paths\n",
    "        self.file_row_counts: List[int] = []\n",
    "        self.file_row_group_counts: List[int] = []\n",
    "        self.row_group_sizes: List[List[int]] = []\n",
    "        for f in file_paths:\n",
    "            pq_file = pq.ParquetFile(f)\n",
    "            self.file_row_counts.append(pq_file.metadata.num_rows)\n",
    "            self.file_row_group_counts.append(pq_file.num_row_groups)\n",
    "            self.row_group_sizes.append([pq_file.row_group(i).num_rows for i in range(self.file_row_group_counts[-1])])\n",
    "            \n",
    "        self.cum_row_counts: List[int] = [sum(self.file_row_counts[:i]) for i in range(len(self.file_row_counts))]\n",
    "\n",
    "        self.pq_df: Optional[pl.DataFrame] = None\n",
    "        self.pq_df_idx: Optional[int] = None \n",
    "        self.pq_df_group_idx: Optional[int] = None\n",
    "\n",
    "        if logging:\n",
    "            print(\"Loaded files with rows:\")\n",
    "            for i, file in enumerate(file_paths):\n",
    "                print(f\"\\t{self.file_row_counts[i]} : {file}\")\n",
    "\n",
    "            print(f\"{len(self)} total samples.\" )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return sum(self.file_row_counts)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    def get_single_row_with_row_group_batching(self, row_idx:int, file_idx: int, row_group_idx: int) -> pl.DataFrame:\n",
    "        if file_idx != self.pq_df_idx or row_group_idx != self.pq_df_group_idx:\n",
    "            self.load_pq_file(idx=file_idx, row_group=row_group_idx)\n",
    "\n",
    "        return self.pq_df.row(row_idx)\n",
    "\n",
    "    def get_single_row_with_batching(self, row_idx: int, file_idx: int) -> pl.DataFrame:\n",
    "        if file_idx != self.pq_df_idx:\n",
    "            self.load_pq_file(idx=file_idx)\n",
    "\n",
    "        return self.pq_df.row(row_idx)\n",
    "\n",
    "    def get_single_row(self, idx: int) -> pl.DataFrame:\n",
    "        # Identify which file and which row within that file corresponds to idx\n",
    "        file_idx: int = self.calculate_file_index_from_global_index(idx)\n",
    "        row_idx: int = idx - self.cum_row_counts[file_idx]\n",
    "        \n",
    "        # Load data for the required row\n",
    "        return self.get_row_from_pq_file(file=self.file_paths[file_idx], row_idx=row_idx)\n",
    "\n",
    "    def calculate_file_index_from_global_index(self, idx: int) -> int:\n",
    "        for i, cum_row_count in enumerate(self.cum_row_counts):\n",
    "            if cum_row_count > idx:\n",
    "                return i-1\n",
    "\n",
    "        if idx < len(self):\n",
    "            return len(self.cum_row_counts)-1\n",
    "        \n",
    "        raise ValueError(f\"Index {idx} too large for dataset with {len(self)} samples\")\n",
    "                \n",
    "    @staticmethod\n",
    "    def get_row_from_pq_file(file: str, row_idx: int):\n",
    "        return pl.read_parquet(file, row_index_offset=row_idx, n_rows=1, use_pyarrow=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_df_row_to_tensor(row: pl.DataFrame) -> torch.Tensor:\n",
    "        # TODO\n",
    "        return row\n",
    "    \n",
    "    def load_pq_file(self, idx: int, row_group: Optional[int] = None) -> None:\n",
    "        if row_group is not None:\n",
    "            self.pq_df_group_idx = row_group\n",
    "            self.pq_df = pl.from_arrow(pq.ParquetFile(self.file_paths[idx]).read_row_group(row_group))\n",
    "        else:\n",
    "            self.pq_df = pl.read_parquet(self.file_paths[idx])\n",
    "\n",
    "        self.pq_df_idx = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T00:45:13.285928Z",
     "iopub.status.busy": "2024-11-11T00:45:13.285505Z",
     "iopub.status.idle": "2024-11-11T00:46:47.707723Z",
     "shell.execute_reply": "2024-11-11T00:46:47.705581Z",
     "shell.execute_reply.started": "2024-11-11T00:45:13.285885Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ParquetFile' object has no attribute 'row_group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m K_TEST_FILES: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet/date_id=0/part-0.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# K_TRAIN_FILE_INDICES: List[int] = [8]\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK_TRAIN_FILES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# test_dataset = ParquetDataset(file_paths=K_TEST_FILES, logging=True)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[103], line 11\u001b[0m, in \u001b[0;36mParquetDataset.__init__\u001b[0;34m(self, file_paths, logging)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_row_counts\u001b[38;5;241m.\u001b[39mappend(pq_file\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mnum_rows)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_row_group_counts\u001b[38;5;241m.\u001b[39mappend(pq_file\u001b[38;5;241m.\u001b[39mnum_row_groups)\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrow_group_sizes\u001b[38;5;241m.\u001b[39mappend([pq_file\u001b[38;5;241m.\u001b[39mrow_group(i)\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_row_group_counts[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcum_row_counts: List[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_row_counts[:i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_row_counts))]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpq_df: Optional[pl\u001b[38;5;241m.\u001b[39mDataFrame] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[103], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_row_counts\u001b[38;5;241m.\u001b[39mappend(pq_file\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mnum_rows)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_row_group_counts\u001b[38;5;241m.\u001b[39mappend(pq_file\u001b[38;5;241m.\u001b[39mnum_row_groups)\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrow_group_sizes\u001b[38;5;241m.\u001b[39mappend([\u001b[43mpq_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrow_group\u001b[49m(i)\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_row_group_counts[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcum_row_counts: List[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_row_counts[:i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_row_counts))]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpq_df: Optional[pl\u001b[38;5;241m.\u001b[39mDataFrame] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ParquetFile' object has no attribute 'row_group'"
     ]
    }
   ],
   "source": [
    "def make_train_parquet_path(i: int) -> str:\n",
    "    return f\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id={i}/part-0.parquet\"\n",
    "# Setup the file indices to use.\n",
    "K_MAX_TRAIN_FILES: int = 10\n",
    "K_TRAIN_FILES: List[str] = [make_train_parquet_path(i) for i in range(K_MAX_TRAIN_FILES)]\n",
    "K_TEST_FILES: List[str] = [\"/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet/date_id=0/part-0.parquet\"]\n",
    "\n",
    "# K_TRAIN_FILE_INDICES: List[int] = [8]\n",
    "\n",
    "train_dataset = ParquetDataset(file_paths=K_TRAIN_FILES, logging=True)\n",
    "# test_dataset = ParquetDataset(file_paths=K_TEST_FILES, logging=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Run timing analysis to check batched approach vs. naive single-row loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing\n",
    "def iterate_samples_by_row(step_size: int):\n",
    "    for i in range(0, len(train_dataset), step_size):\n",
    "        train_dataset.get_single_row(i)\n",
    "\n",
    "@timing\n",
    "def iterate_samples_batched(step_size: int):\n",
    "    for file_idx in range(K_MAX_TRAIN_FILES):\n",
    "        for row_idx in range(0, train_dataset.file_row_counts[file_idx], step_size):\n",
    "            train_dataset.get_single_row_with_batching(row_idx=row_idx, file_idx=file_idx)\n",
    "\n",
    "# A simple batching schemem is definitely the way to go. We can iterate orders of magnitude more samples\n",
    "# in the same amount of time it would take to do naive random access.\n",
    "iterate_samples_by_row(step_size=1000000)\n",
    "iterate_samples_by_row(step_size=100000)\n",
    "iterate_samples_by_row(step_size=10000)\n",
    "iterate_samples_batched(step_size=1000)\n",
    "iterate_samples_batched(step_size=100)\n",
    "iterate_samples_batched(step_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a custom sample for randomly ordering the parquet files and rows within the parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParquetBatchedSampler(Sampler[int]):\n",
    "    r\"\"\"Samples elements randomly while batching by Parquet file to minimize disk i/o.\n",
    "    Randomly orders the N parquet files and then provides random indices into the rows of each parquet file,\n",
    "    ensuring that each file is fully sampled before movign on the to next.\n",
    "\n",
    "    Args:\n",
    "        data_source (Dataset): dataset to sample from\n",
    "        generator (Generator): Generator used in sampling.\n",
    "    \"\"\"\n",
    "\n",
    "    data_source: ParquetDataset\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_source: ParquetDataset,\n",
    "        generator=None,\n",
    "    ) -> None:\n",
    "        self.data_source = data_source\n",
    "        self.generator = generator\n",
    "\n",
    "    @property\n",
    "    def num_samples(self) -> int:\n",
    "        # dataset size might change at runtime\n",
    "        if self._num_samples is None:\n",
    "            return len(self.data_source)\n",
    "        return self._num_samples\n",
    "\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        n = len(self.data_source)\n",
    "        if self.generator is None:\n",
    "            seed = int(torch.empty((), dtype=torch.int64).random_().item())\n",
    "            generator = torch.Generator()\n",
    "            generator.manual_seed(seed)\n",
    "        else:\n",
    "            generator = self.generator\n",
    "\n",
    "        # Randomly sample from an ordering or partitions, shuffling all of the samples within a partition.\n",
    "        for file_idx in torch.randperm(len(self.data_source.file_paths), generator=generator):\n",
    "            offset: int = self.data_source.cum_row_counts[file_idx]\n",
    "            yield from offset + torch.randperm(self.data_source.file_row_counts[file_idx], generator=generator)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Verify that the sampler produces samples in random parition order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4) 11802114\n",
      "0 of 47127338\n",
      "found new file 4\n",
      "1000000 of 47127338\n",
      "2000000 of 47127338\n",
      "3000000 of 47127338\n",
      "4000000 of 47127338\n",
      "5000000 of 47127338\n",
      "tensor(7) 28377178\n",
      "found new file 7\n",
      "6000000 of 47127338\n",
      "7000000 of 47127338\n",
      "8000000 of 47127338\n",
      "9000000 of 47127338\n",
      "10000000 of 47127338\n",
      "11000000 of 47127338\n",
      "tensor(1) 1944210\n",
      "found new file 1\n",
      "12000000 of 47127338\n",
      "13000000 of 47127338\n",
      "14000000 of 47127338\n",
      "tensor(9) 40852762\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Index 45835731 too large for dataset with 47127338 samples",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sampler)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m file_idx \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_file_index_from_global_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m curr_partition \u001b[38;5;241m!=\u001b[39m file_idx:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# update the partition index assuming that once we see a file index and then leave it\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# we'll never see it again. Throw an error if we do see it again.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_idx \u001b[38;5;129;01min\u001b[39;00m partition_ordering:\n",
      "Cell \u001b[0;32mIn[96], line 42\u001b[0m, in \u001b[0;36mParquetDataset.calculate_file_index_from_global_index\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cum_row_count \u001b[38;5;241m>\u001b[39m idx:\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m too large for dataset with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Index 45835731 too large for dataset with 47127338 samples"
     ]
    }
   ],
   "source": [
    "sampler = ParquetBatchedSampler(data_source=train_dataset)\n",
    "partition_ordering: List[int] = []\n",
    "num_sampled_per_partition: List[int] = []\n",
    "curr_partition: Optional[int] = None\n",
    "for i, idx in enumerate(sampler):\n",
    "    if (i % 1000000 == 0):\n",
    "        print(f\"{i} of {len(sampler)}\\r\")\n",
    "    file_idx = train_dataset.calculate_file_index_from_global_index(idx)\n",
    "    if curr_partition != file_idx:\n",
    "        # update the partition index assuming that once we see a file index and then leave it\n",
    "        # we'll never see it again. Throw an error if we do see it again.\n",
    "        if file_idx in partition_ordering:\n",
    "            raise ValueError(f\"File index {file_idx} seen out of order! Already seen ordering: {partition_ordering}\")\n",
    "        print(f\"found new file {file_idx}\")\n",
    "        curr_partition = file_idx\n",
    "        partition_ordering.append(curr_partition)\n",
    "        if len(num_sampled_per_partition) > 0:\n",
    "            num_sampled_per_partition.append(i - num_sampled_per_partition[-1])\n",
    "        else:\n",
    "            num_sampled_per_partition.append(i)\n",
    "\n",
    "print(f\"Total elements: {len(sampler)}. Sampled partition ordering: {partition_ordering}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sampled_per_partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T21:17:16.113402Z",
     "iopub.status.busy": "2024-11-10T21:17:16.112741Z",
     "iopub.status.idle": "2024-11-10T21:17:19.029436Z",
     "shell.execute_reply": "2024-11-10T21:17:19.028057Z",
     "shell.execute_reply.started": "2024-11-10T21:17:16.113331Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "# Separate features and responders\n",
    "features = sample_df.filter(regex='^feature_')\n",
    "responders = sample_df.filter(regex='^responder_')\n",
    "# Convert to numpy arrays for TensorFlow\n",
    "X = features.values  # Features for input\n",
    "y = responders.values  # Responders for output\n",
    "X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "y = np.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T21:17:19.034077Z",
     "iopub.status.busy": "2024-11-10T21:17:19.033604Z",
     "iopub.status.idle": "2024-11-10T21:17:19.098752Z",
     "shell.execute_reply": "2024-11-10T21:17:19.097454Z",
     "shell.execute_reply.started": "2024-11-10T21:17:19.034029Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the number of input and output nodes\n",
    "input_dim = X.shape[1]  # Number of features (79)\n",
    "output_dim = y.shape[1]  # Number of responders (9)\n",
    "# Define the model\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(input_dim,)),  # Input layer\n",
    "    layers.Dense(64, activation='relu'),  # Encoder\n",
    "    layers.Dense(32, activation='relu'),  # Bottleneck layer (compression)\n",
    "    layers.Dense(64, activation='relu'),  # Decoder\n",
    "    layers.Dense(output_dim, activation='linear')  # Output layer for responders\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T21:17:19.100583Z",
     "iopub.status.busy": "2024-11-10T21:17:19.100159Z",
     "iopub.status.idle": "2024-11-10T21:17:19.106868Z",
     "shell.execute_reply": "2024-11-10T21:17:19.105404Z",
     "shell.execute_reply.started": "2024-11-10T21:17:19.100541Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "def step_decay(epoch):\n",
    "    initial_lr = 0.01\n",
    "    drop = 0.5\n",
    "    epochs_drop = 5\n",
    "    lr = initial_lr * (drop ** (epoch // epochs_drop))\n",
    "    return lr\n",
    "lr_scheduler = LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T21:17:19.109203Z",
     "iopub.status.busy": "2024-11-10T21:17:19.108678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Define EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',    # Monitor validation loss\n",
    "    patience=10,            # Number of epochs to wait for improvement\n",
    "    min_delta=0.001,       # Minimum change to qualify as an improvement\n",
    "    restore_best_weights=True  # Restore weights from the best epoch\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X, y,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save(\"/kaggle/working/model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "See [Jane Street RMF Demo Submission](https://www.kaggle.com/code/ryanholbrook/jane-street-rmf-demo-submission) for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import kaggle_evaluation.jane_street_inference_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "# Assuming `model` is your trained model\n",
    "# Assuming features required by the model are named 'feature_00', 'feature_01', etc.\n",
    "def predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Make a prediction.\"\"\"\n",
    "    global lags_\n",
    "    if lags is not None:\n",
    "        lags_ = lags\n",
    "    # Extract the features for the model input\n",
    "    feature_columns = [col for col in test.columns if col.startswith(\"feature_\")]\n",
    "    features = test.select(feature_columns).to_numpy()  # Convert to numpy array for model input\n",
    "    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    # Generate predictions using the model\n",
    "    model_predictions = model.predict(features)\n",
    "    responder_6_predictions = model_predictions[:, 6]  # Assuming responder_6 is at index 6\n",
    "    # Create a new Polars DataFrame with row_id and responder_6 predictions\n",
    "    predictions = test.select(\"row_id\").with_columns(\n",
    "        pl.Series(\"responder_6\", responder_6_predictions)\n",
    "    )\n",
    "    # Ensure the output format and length requirements\n",
    "    if isinstance(predictions, pl.DataFrame):\n",
    "        assert predictions.columns == ['row_id', 'responder_6']\n",
    "    elif isinstance(predictions, pd.DataFrame):\n",
    "        assert (predictions.columns == ['row_id', 'responder_6']).all()\n",
    "    else:\n",
    "        raise TypeError('The predict function must return a DataFrame')\n",
    "    \n",
    "    assert len(predictions) == len(test)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inference_server = kaggle_evaluation.jane_street_inference_server.JSInferenceServer(predict)\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "            '/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet',\n",
    "            '/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet',\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9871156,
     "sourceId": 84493,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
