{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Callable, Iterator, Tuple\n",
    "import polars as pl\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "from torch.utils.data import Dataset, Sampler, DataLoader\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "from time import time\n",
    "\n",
    "def timing(f: Callable, *args, **kwargs) -> Callable:\n",
    "    @wraps(f)\n",
    "    def wrap(*args, **kwargs):\n",
    "        ts = time()\n",
    "        result = f(*args, **kwargs)\n",
    "        te = time()\n",
    "        print('func:%r args:[%r, %r] took: %2.4f sec' % \\\n",
    "          (f.__name__, args, kwargs, te-ts))\n",
    "        return result\n",
    "    return wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset for the Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParquetDataset(Dataset):\n",
    "    def __init__(self, file_paths: List[str], batch_size: int = 1000, logging=True):\n",
    "        self.file_paths = file_paths\n",
    "        self.file_row_counts: List[int] = []  \n",
    "        self.file_row_group_counts: List[List[int]] = []\n",
    "        for f in file_paths:\n",
    "            pq_file = pq.ParquetFile(f)\n",
    "            self.file_row_group_counts.append([pq_file.metadata.row_group(i).num_rows for i in range(pq_file.num_row_groups)]) \n",
    "            self.file_row_counts.append(sum(self.file_row_group_counts[-1]))\n",
    "\n",
    "        self.cum_row_counts: List[int] = np.array([sum(self.file_row_counts[:i]) for i in range(len(self.file_row_counts))])\n",
    "        self.cum_row_group_counts: List[List[int]] = [np.array([sum(c[:i]) for i in range(len(c))]) for c in self.file_row_group_counts]\n",
    "\n",
    "        self.cum_total_counts: List[Tuple[int, int, int]] = []\n",
    "        for f in range(len(self.cum_row_counts)):\n",
    "            for r in range(len(self.cum_row_group_counts[f])):\n",
    "                self.cum_total_counts.append((f, r, self.cum_row_counts[f] + self.cum_row_group_counts[f][r]))\n",
    "        self.cum_total_counts_np = np.array([v[2] for v in self.cum_total_counts])\n",
    "\n",
    "        self.pq_df: Optional[pl.DataFrame] = None\n",
    "        self.pq_df_idx: Optional[int] = None\n",
    "        self.pq_df_batch_idx: Optional[int] = None \n",
    "\n",
    "        if logging:\n",
    "            print(\"Loaded files with rows:\")\n",
    "            for i, file in enumerate(file_paths):\n",
    "                print(f\"\\t{self.file_row_counts[i]} : {file}\")\n",
    "\n",
    "            print(f\"{len(self)} total samples.\" )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return sum(self.file_row_counts)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    def get_single_row_with_row_group_batching(self, row_idx:int, file_idx: int, row_group_idx: int) -> pl.DataFrame:\n",
    "        if file_idx != self.pq_df_idx or row_group_idx != self.pq_df_group_idx:\n",
    "            self.load_pq_file(idx=file_idx, row_group=row_group_idx)\n",
    "\n",
    "        return self.pq_df.row(row_idx)\n",
    "\n",
    "    def get_single_row_with_batching(self, row_idx: int, file_idx: int) -> pl.DataFrame:\n",
    "        if file_idx != self.pq_df_idx:\n",
    "            self.load_pq_file(idx=file_idx)\n",
    "\n",
    "        return self.pq_df.row(row_idx)\n",
    "\n",
    "    def get_single_row(self, idx: int) -> pl.DataFrame:\n",
    "        # Identify which file and which row within that file corresponds to idx\n",
    "        file_idx: int = self.calculate_index_from_cumulative_counts(idx, self.cum_row_counts)\n",
    "        row_idx: int = idx - self.cum_row_counts[file_idx]\n",
    "        \n",
    "        # Load data for the required row\n",
    "        return self.get_row_from_pq_file(file=self.file_paths[file_idx], row_idx=row_idx)\n",
    "        \n",
    "    \n",
    "    def calculate_file_and_row_group_index_from_global_index(self, idx: int) -> Tuple[int, int]:\n",
    "        # Get the file index for this global index.\n",
    "        file_index: int = self.calculate_index_from_cumulative_counts(idx, self.cum_row_counts)\n",
    "\n",
    "        # Get the row index within this file and then the row group within that file.\n",
    "        row_index = idx - self.cum_row_counts[file_index]\n",
    "        row_group_idx = self.calculate_index_from_cumulative_counts(\n",
    "            idx=row_index,\n",
    "            cumulative_counts=self.cum_row_group_counts[file_index]\n",
    "        )\n",
    "        \n",
    "        # Return a tuple of file idx and row group idx\n",
    "        return (file_index, row_group_idx)\n",
    "        \n",
    "        \n",
    "    def calculate_index_from_cumulative_counts(self, idx: int, cumulative_counts: np.ndarray) -> int:\n",
    "        return np.searchsorted(cumulative_counts, idx, side=\"right\") - 1\n",
    "        \n",
    "                \n",
    "    @staticmethod\n",
    "    def get_row_from_pq_file(file: str, row_idx: int):\n",
    "        return pl.read_parquet(file, row_index_offset=row_idx, n_rows=1, use_pyarrow=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_df_row_to_tensor(row: pl.DataFrame) -> torch.Tensor:\n",
    "        # TODO\n",
    "        return row\n",
    "    \n",
    "    def load_pq_file(self, idx: int, row_group: Optional[int] = None) -> None:\n",
    "        if row_group is not None:\n",
    "            self.pq_df_group_idx = row_group\n",
    "            self.pq_df = pl.from_arrow(pq.ParquetFile(self.file_paths[idx]).read_row_group(row_group))\n",
    "        else:\n",
    "            self.pq_df = pl.read_parquet(self.file_paths[idx])\n",
    "\n",
    "        self.pq_df_idx = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T00:45:13.285928Z",
     "iopub.status.busy": "2024-11-11T00:45:13.285505Z",
     "iopub.status.idle": "2024-11-11T00:46:47.707723Z",
     "shell.execute_reply": "2024-11-11T00:46:47.705581Z",
     "shell.execute_reply.started": "2024-11-11T00:45:13.285885Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded files with rows:\n",
      "\t1944210 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0/part-0.parquet\n",
      "\t2804247 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=1/part-0.parquet\n",
      "\t3036873 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=2/part-0.parquet\n",
      "\t4016784 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=3/part-0.parquet\n",
      "\t5022952 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=4/part-0.parquet\n",
      "\t5348200 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=5/part-0.parquet\n",
      "\t6203912 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=6/part-0.parquet\n",
      "\t6335560 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=7/part-0.parquet\n",
      "\t6140024 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=8/part-0.parquet\n",
      "\t6274576 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=9/part-0.parquet\n",
      "47127338 total samples.\n"
     ]
    }
   ],
   "source": [
    "def make_train_parquet_path(i: int) -> str:\n",
    "    return f\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id={i}/part-0.parquet\"\n",
    "# Setup the file indices to use.\n",
    "K_MAX_TRAIN_FILES: int = 10\n",
    "K_TRAIN_FILES: List[str] = [make_train_parquet_path(i) for i in range(K_MAX_TRAIN_FILES)]\n",
    "K_TEST_FILES: List[str] = [\"/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet/date_id=0/part-0.parquet\"]\n",
    "\n",
    "# K_TRAIN_FILE_INDICES: List[int] = [8]\n",
    "\n",
    "train_dataset = ParquetDataset(file_paths=K_TRAIN_FILES, logging=True)\n",
    "# test_dataset = ParquetDataset(file_paths=K_TEST_FILES, logging=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Run timing analysis to check batched approach vs. naive single-row loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:'iterate_samples_by_row' args:[(), {'step_size': 1000000}] took: 4.5942 sec\n",
      "func:'iterate_samples_by_row' args:[(), {'step_size': 100000}] took: 35.9613 sec\n",
      "func:'iterate_samples_batched' args:[(), {'step_size': 1000}] took: 10.0974 sec\n",
      "func:'iterate_samples_batched' args:[(), {'step_size': 100}] took: 16.4546 sec\n",
      "func:'iterate_samples_batched_row_group' args:[(), {'step_size': 1000}] took: 15.8445 sec\n",
      "func:'iterate_samples_batched_row_group' args:[(), {'step_size': 100}] took: 22.0621 sec\n"
     ]
    }
   ],
   "source": [
    "@timing\n",
    "def iterate_samples_by_row(step_size: int):\n",
    "    for i in range(0, len(train_dataset), step_size):\n",
    "        train_dataset.get_single_row(i)\n",
    "\n",
    "@timing\n",
    "def iterate_samples_batched(step_size: int):\n",
    "    for i in range(0, len(train_dataset), step_size):\n",
    "        file_idx = train_dataset.calculate_index_from_cumulative_counts(i, train_dataset.cum_row_counts)\n",
    "        row_idx = i - train_dataset.cum_row_counts[file_idx]\n",
    "        train_dataset.get_single_row_with_batching(row_idx=row_idx, file_idx=file_idx)\n",
    "\n",
    "@timing\n",
    "def iterate_samples_batched_row_group(step_size: int):\n",
    "    for i in range(0, len(train_dataset), step_size):\n",
    "        file_idx, row_group_idx = train_dataset.calculate_file_and_row_group_index_from_global_index(i)\n",
    "        row_idx = i - train_dataset.cum_row_counts[file_idx] - train_dataset.cum_row_group_counts[file_idx][row_group_idx]\n",
    "        train_dataset.get_single_row_with_row_group_batching(row_idx=row_idx, file_idx=file_idx, row_group_idx=row_group_idx)\n",
    "\n",
    "# A simple batching schemem is definitely the way to go. We can iterate orders of magnitude more samples\n",
    "# in the same amount of time it would take to do naive random access.\n",
    "iterate_samples_by_row(step_size=1000000)\n",
    "iterate_samples_by_row(step_size=100000)\n",
    "\n",
    "iterate_samples_batched(step_size=1000)\n",
    "iterate_samples_batched(step_size=100)\n",
    "\n",
    "iterate_samples_batched_row_group(step_size=1000)\n",
    "iterate_samples_batched_row_group(step_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a custom sample for randomly ordering the parquet files and rows within the parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParquetBatchedSampler(Sampler[int]):\n",
    "    r\"\"\"Samples elements randomly while batching by Parquet file to minimize disk i/o.\n",
    "    Randomly orders the N parquet files and then provides random indices into the rows of each parquet file,\n",
    "    ensuring that each file is fully sampled before movign on the to next.\n",
    "\n",
    "    Args:\n",
    "        data_source (Dataset): dataset to sample from\n",
    "        generator (Generator): Generator used in sampling.\n",
    "    \"\"\"\n",
    "\n",
    "    data_source: ParquetDataset\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_source: ParquetDataset,\n",
    "        generator=None,\n",
    "    ) -> None:\n",
    "        self.data_source = data_source\n",
    "        self.generator = generator\n",
    "\n",
    "    @property\n",
    "    def num_samples(self) -> int:\n",
    "        # dataset size might change at runtime\n",
    "        if self._num_samples is None:\n",
    "            return len(self.data_source)\n",
    "        return self._num_samples\n",
    "\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "\n",
    "        if self.generator is None:\n",
    "            seed = int(torch.empty((), dtype=torch.int64).random_().item())\n",
    "            generator = torch.Generator()\n",
    "            generator.manual_seed(seed)\n",
    "        else:\n",
    "            generator = self.generator\n",
    "\n",
    "        # Randomly sample from an ordering or partitions, shuffling all of the samples within a partition.\n",
    "        for i in torch.randperm(len(self.data_source.cum_total_counts), generator=generator):\n",
    "            file_idx, row_group_idx, offset = self.data_source.cum_total_counts[i]\n",
    "            yield from offset + torch.randperm(self.data_source.file_row_group_counts[file_idx][row_group_idx])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Verify that the sampler produces samples in random parition order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 47127338\n",
      "found new file 21\n",
      "found new file 165\n",
      "found new file 58\n",
      "found new file 16\n",
      "1000000 of 47127338\n",
      "found new file 32\n",
      "found new file 50\n",
      "found new file 18\n",
      "found new file 59\n",
      "2000000 of 47127338\n",
      "found new file 89\n",
      "found new file 53\n",
      "found new file 107\n",
      "found new file 66\n",
      "3000000 of 47127338\n",
      "found new file 160\n",
      "found new file 68\n",
      "found new file 148\n",
      "4000000 of 47127338\n",
      "found new file 124\n",
      "found new file 33\n",
      "found new file 125\n",
      "found new file 26\n",
      "5000000 of 47127338\n",
      "found new file 141\n",
      "found new file 24\n",
      "found new file 37\n",
      "found new file 25\n",
      "6000000 of 47127338\n",
      "found new file 172\n",
      "found new file 45\n",
      "found new file 132\n",
      "7000000 of 47127338\n",
      "found new file 19\n",
      "found new file 140\n",
      "found new file 4\n",
      "found new file 57\n",
      "8000000 of 47127338\n",
      "found new file 87\n",
      "found new file 48\n",
      "found new file 70\n",
      "found new file 163\n",
      "9000000 of 47127338\n",
      "found new file 62\n",
      "found new file 61\n",
      "found new file 171\n",
      "found new file 46\n",
      "10000000 of 47127338\n",
      "found new file 49\n",
      "found new file 10\n",
      "found new file 30\n",
      "11000000 of 47127338\n",
      "found new file 6\n",
      "found new file 158\n",
      "found new file 47\n",
      "found new file 129\n",
      "12000000 of 47127338\n",
      "found new file 64\n",
      "found new file 8\n",
      "found new file 9\n",
      "found new file 27\n",
      "13000000 of 47127338\n",
      "found new file 134\n",
      "found new file 122\n",
      "found new file 41\n",
      "14000000 of 47127338\n",
      "found new file 40\n",
      "found new file 82\n",
      "found new file 121\n",
      "found new file 28\n",
      "15000000 of 47127338\n",
      "found new file 51\n",
      "found new file 120\n",
      "found new file 84\n",
      "found new file 88\n",
      "16000000 of 47127338\n",
      "found new file 83\n",
      "found new file 103\n",
      "found new file 146\n",
      "found new file 65\n",
      "17000000 of 47127338\n",
      "found new file 54\n",
      "found new file 108\n",
      "found new file 36\n",
      "18000000 of 47127338\n",
      "found new file 97\n",
      "found new file 135\n",
      "found new file 55\n",
      "found new file 67\n",
      "19000000 of 47127338\n",
      "found new file 153\n",
      "found new file 130\n",
      "found new file 0\n",
      "found new file 100\n",
      "20000000 of 47127338\n",
      "found new file 14\n",
      "found new file 162\n",
      "found new file 90\n",
      "21000000 of 47127338\n",
      "found new file 102\n",
      "found new file 137\n",
      "found new file 127\n",
      "found new file 118\n",
      "22000000 of 47127338\n",
      "found new file 23\n",
      "found new file 60\n",
      "found new file 73\n",
      "found new file 2\n",
      "23000000 of 47127338\n",
      "found new file 106\n",
      "found new file 114\n",
      "found new file 78\n",
      "found new file 109\n",
      "24000000 of 47127338\n",
      "found new file 94\n",
      "found new file 112\n",
      "found new file 161\n",
      "25000000 of 47127338\n",
      "found new file 91\n",
      "found new file 167\n",
      "found new file 76\n",
      "found new file 81\n",
      "26000000 of 47127338\n",
      "found new file 116\n",
      "found new file 136\n",
      "found new file 34\n",
      "found new file 1\n",
      "27000000 of 47127338\n",
      "found new file 131\n",
      "found new file 74\n",
      "found new file 69\n",
      "found new file 164\n",
      "28000000 of 47127338\n",
      "found new file 93\n",
      "found new file 43\n",
      "found new file 139\n",
      "29000000 of 47127338\n",
      "found new file 85\n",
      "found new file 173\n",
      "found new file 13\n",
      "found new file 170\n",
      "30000000 of 47127338\n",
      "found new file 79\n",
      "found new file 151\n",
      "found new file 75\n",
      "found new file 113\n",
      "31000000 of 47127338\n",
      "found new file 149\n",
      "found new file 147\n",
      "found new file 150\n",
      "32000000 of 47127338\n",
      "found new file 22\n",
      "found new file 145\n",
      "found new file 152\n",
      "found new file 71\n",
      "33000000 of 47127338\n",
      "found new file 20\n",
      "found new file 96\n",
      "found new file 98\n",
      "found new file 99\n",
      "34000000 of 47127338\n",
      "found new file 104\n",
      "found new file 5\n",
      "found new file 35\n",
      "35000000 of 47127338\n",
      "found new file 143\n",
      "found new file 29\n",
      "found new file 11\n",
      "found new file 44\n",
      "36000000 of 47127338\n",
      "found new file 168\n",
      "found new file 123\n",
      "found new file 31\n",
      "found new file 128\n",
      "37000000 of 47127338\n",
      "found new file 3\n",
      "found new file 42\n",
      "found new file 72\n",
      "found new file 115\n",
      "38000000 of 47127338\n",
      "found new file 12\n",
      "found new file 15\n",
      "found new file 105\n",
      "39000000 of 47127338\n",
      "found new file 157\n",
      "found new file 138\n",
      "found new file 39\n",
      "found new file 101\n",
      "40000000 of 47127338\n",
      "found new file 86\n",
      "found new file 174\n",
      "found new file 119\n",
      "found new file 166\n",
      "41000000 of 47127338\n",
      "found new file 92\n",
      "found new file 95\n",
      "found new file 7\n",
      "42000000 of 47127338\n",
      "found new file 56\n",
      "found new file 17\n",
      "found new file 144\n",
      "found new file 142\n",
      "43000000 of 47127338\n",
      "found new file 169\n",
      "found new file 159\n",
      "found new file 155\n",
      "found new file 63\n",
      "44000000 of 47127338\n",
      "found new file 52\n",
      "found new file 38\n",
      "found new file 156\n",
      "found new file 111\n",
      "45000000 of 47127338\n",
      "found new file 133\n",
      "found new file 77\n",
      "found new file 110\n",
      "46000000 of 47127338\n",
      "found new file 117\n",
      "found new file 80\n",
      "found new file 126\n",
      "found new file 154\n",
      "47000000 of 47127338\n",
      "Total elements: 47127338. Sampled partition ordering: [np.int64(21), np.int64(165), np.int64(58), np.int64(16), np.int64(32), np.int64(50), np.int64(18), np.int64(59), np.int64(89), np.int64(53), np.int64(107), np.int64(66), np.int64(160), np.int64(68), np.int64(148), np.int64(124), np.int64(33), np.int64(125), np.int64(26), np.int64(141), np.int64(24), np.int64(37), np.int64(25), np.int64(172), np.int64(45), np.int64(132), np.int64(19), np.int64(140), np.int64(4), np.int64(57), np.int64(87), np.int64(48), np.int64(70), np.int64(163), np.int64(62), np.int64(61), np.int64(171), np.int64(46), np.int64(49), np.int64(10), np.int64(30), np.int64(6), np.int64(158), np.int64(47), np.int64(129), np.int64(64), np.int64(8), np.int64(9), np.int64(27), np.int64(134), np.int64(122), np.int64(41), np.int64(40), np.int64(82), np.int64(121), np.int64(28), np.int64(51), np.int64(120), np.int64(84), np.int64(88), np.int64(83), np.int64(103), np.int64(146), np.int64(65), np.int64(54), np.int64(108), np.int64(36), np.int64(97), np.int64(135), np.int64(55), np.int64(67), np.int64(153), np.int64(130), np.int64(0), np.int64(100), np.int64(14), np.int64(162), np.int64(90), np.int64(102), np.int64(137), np.int64(127), np.int64(118), np.int64(23), np.int64(60), np.int64(73), np.int64(2), np.int64(106), np.int64(114), np.int64(78), np.int64(109), np.int64(94), np.int64(112), np.int64(161), np.int64(91), np.int64(167), np.int64(76), np.int64(81), np.int64(116), np.int64(136), np.int64(34), np.int64(1), np.int64(131), np.int64(74), np.int64(69), np.int64(164), np.int64(93), np.int64(43), np.int64(139), np.int64(85), np.int64(173), np.int64(13), np.int64(170), np.int64(79), np.int64(151), np.int64(75), np.int64(113), np.int64(149), np.int64(147), np.int64(150), np.int64(22), np.int64(145), np.int64(152), np.int64(71), np.int64(20), np.int64(96), np.int64(98), np.int64(99), np.int64(104), np.int64(5), np.int64(35), np.int64(143), np.int64(29), np.int64(11), np.int64(44), np.int64(168), np.int64(123), np.int64(31), np.int64(128), np.int64(3), np.int64(42), np.int64(72), np.int64(115), np.int64(12), np.int64(15), np.int64(105), np.int64(157), np.int64(138), np.int64(39), np.int64(101), np.int64(86), np.int64(174), np.int64(119), np.int64(166), np.int64(92), np.int64(95), np.int64(7), np.int64(56), np.int64(17), np.int64(144), np.int64(142), np.int64(169), np.int64(159), np.int64(155), np.int64(63), np.int64(52), np.int64(38), np.int64(156), np.int64(111), np.int64(133), np.int64(77), np.int64(110), np.int64(117), np.int64(80), np.int64(126), np.int64(154)]\n"
     ]
    }
   ],
   "source": [
    "sampler = ParquetBatchedSampler(data_source=train_dataset)\n",
    "partition_ordering: List[Tuple[int,int]] = []\n",
    "num_sampled_per_partition: List[int] = []\n",
    "curr_partition: Optional[Tuple[int, int]] = None\n",
    "\n",
    "for i, idx in enumerate(sampler):\n",
    "\n",
    "    if (i % 1000000 == 0):\n",
    "        print(f\"{i} of {len(sampler)}\\r\")\n",
    "\n",
    "    new_idxes = train_dataset.calculate_index_from_cumulative_counts(idx, train_dataset.cum_total_counts_np)\n",
    "\n",
    "    if curr_partition != new_idxes:\n",
    "        # update the partition index assuming that once we see a file index and then leave it\n",
    "        # we'll never see it again. Throw an error if we do see it again.\n",
    "        if new_idxes in partition_ordering:\n",
    "            raise ValueError(f\"File index {new_idxes} seen out of order! Already seen ordering: {partition_ordering}\")\n",
    "        print(f\"found new file {new_idxes}\")\n",
    "        curr_partition = new_idxes\n",
    "        partition_ordering.append(curr_partition)\n",
    "        if len(num_sampled_per_partition) > 0:\n",
    "            num_sampled_per_partition.append(i - num_sampled_per_partition[-1])\n",
    "        else:\n",
    "            num_sampled_per_partition.append(i)\n",
    "\n",
    "print(f\"Total elements: {len(sampler)}. Sampled partition ordering: {partition_ordering}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Benchmarking how long it takes to serve samples from the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[236], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@timing\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21miterate_dataloader\u001b[39m():\n\u001b[1;32m      3\u001b[0m     dl \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, prefetch_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, sampler\u001b[38;5;241m=\u001b[39mParquetBatchedSampler(data_source\u001b[38;5;241m=\u001b[39mtrain_dataset))\n\u001b[0;32m----> 5\u001b[0m \u001b[43miterate_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[149], line 7\u001b[0m, in \u001b[0;36mtiming.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m----> 7\u001b[0m     ts \u001b[38;5;241m=\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     result \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m      9\u001b[0m     te \u001b[38;5;241m=\u001b[39m time()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "@timing\n",
    "def iterate_dataloader(num_workers=24, batch_size=64, prefetch_factor=10):\n",
    "    dl = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True, prefetch_factor=prefetch_factor, sampler=ParquetBatchedSampler(data_source=train_dataset))\n",
    "\n",
    "iterate_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T21:17:16.113402Z",
     "iopub.status.busy": "2024-11-10T21:17:16.112741Z",
     "iopub.status.idle": "2024-11-10T21:17:19.029436Z",
     "shell.execute_reply": "2024-11-10T21:17:19.028057Z",
     "shell.execute_reply.started": "2024-11-10T21:17:16.113331Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "# Separate features and responders\n",
    "features = sample_df.filter(regex='^feature_')\n",
    "responders = sample_df.filter(regex='^responder_')\n",
    "# Convert to numpy arrays for TensorFlow\n",
    "X = features.values  # Features for input\n",
    "y = responders.values  # Responders for output\n",
    "X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "y = np.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T21:17:19.034077Z",
     "iopub.status.busy": "2024-11-10T21:17:19.033604Z",
     "iopub.status.idle": "2024-11-10T21:17:19.098752Z",
     "shell.execute_reply": "2024-11-10T21:17:19.097454Z",
     "shell.execute_reply.started": "2024-11-10T21:17:19.034029Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the number of input and output nodes\n",
    "input_dim = X.shape[1]  # Number of features (79)\n",
    "output_dim = y.shape[1]  # Number of responders (9)\n",
    "# Define the model\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(input_dim,)),  # Input layer\n",
    "    layers.Dense(64, activation='relu'),  # Encoder\n",
    "    layers.Dense(32, activation='relu'),  # Bottleneck layer (compression)\n",
    "    layers.Dense(64, activation='relu'),  # Decoder\n",
    "    layers.Dense(output_dim, activation='linear')  # Output layer for responders\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T21:17:19.100583Z",
     "iopub.status.busy": "2024-11-10T21:17:19.100159Z",
     "iopub.status.idle": "2024-11-10T21:17:19.106868Z",
     "shell.execute_reply": "2024-11-10T21:17:19.105404Z",
     "shell.execute_reply.started": "2024-11-10T21:17:19.100541Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "def step_decay(epoch):\n",
    "    initial_lr = 0.01\n",
    "    drop = 0.5\n",
    "    epochs_drop = 5\n",
    "    lr = initial_lr * (drop ** (epoch // epochs_drop))\n",
    "    return lr\n",
    "lr_scheduler = LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T21:17:19.109203Z",
     "iopub.status.busy": "2024-11-10T21:17:19.108678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Define EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',    # Monitor validation loss\n",
    "    patience=10,            # Number of epochs to wait for improvement\n",
    "    min_delta=0.001,       # Minimum change to qualify as an improvement\n",
    "    restore_best_weights=True  # Restore weights from the best epoch\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X, y,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save(\"/kaggle/working/model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "See [Jane Street RMF Demo Submission](https://www.kaggle.com/code/ryanholbrook/jane-street-rmf-demo-submission) for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import kaggle_evaluation.jane_street_inference_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "# Assuming `model` is your trained model\n",
    "# Assuming features required by the model are named 'feature_00', 'feature_01', etc.\n",
    "def predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Make a prediction.\"\"\"\n",
    "    global lags_\n",
    "    if lags is not None:\n",
    "        lags_ = lags\n",
    "    # Extract the features for the model input\n",
    "    feature_columns = [col for col in test.columns if col.startswith(\"feature_\")]\n",
    "    features = test.select(feature_columns).to_numpy()  # Convert to numpy array for model input\n",
    "    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    # Generate predictions using the model\n",
    "    model_predictions = model.predict(features)\n",
    "    responder_6_predictions = model_predictions[:, 6]  # Assuming responder_6 is at index 6\n",
    "    # Create a new Polars DataFrame with row_id and responder_6 predictions\n",
    "    predictions = test.select(\"row_id\").with_columns(\n",
    "        pl.Series(\"responder_6\", responder_6_predictions)\n",
    "    )\n",
    "    # Ensure the output format and length requirements\n",
    "    if isinstance(predictions, pl.DataFrame):\n",
    "        assert predictions.columns == ['row_id', 'responder_6']\n",
    "    elif isinstance(predictions, pd.DataFrame):\n",
    "        assert (predictions.columns == ['row_id', 'responder_6']).all()\n",
    "    else:\n",
    "        raise TypeError('The predict function must return a DataFrame')\n",
    "    \n",
    "    assert len(predictions) == len(test)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inference_server = kaggle_evaluation.jane_street_inference_server.JSInferenceServer(predict)\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "            '/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet',\n",
    "            '/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet',\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9871156,
     "sourceId": 84493,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
