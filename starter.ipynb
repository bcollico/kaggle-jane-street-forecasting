{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Callable, Iterator, Tuple\n",
    "import torch\n",
    "from torch.utils.data import Sampler, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "from time import time\n",
    "\n",
    "def timing(f: Callable, *args, **kwargs) -> Callable:\n",
    "    @wraps(f)\n",
    "    def wrap(*args, **kwargs):\n",
    "        ts = time()\n",
    "        result = f(*args, **kwargs)\n",
    "        te = time()\n",
    "        print('func:%r args:[%r, %r] took: %2.4f sec' % \\\n",
    "          (f.__name__, args, kwargs, te-ts))\n",
    "        return result\n",
    "    return wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset for the Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parquet_dataset.parquet_dataset import ParquetDataset\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T00:45:13.285928Z",
     "iopub.status.busy": "2024-11-11T00:45:13.285505Z",
     "iopub.status.idle": "2024-11-11T00:46:47.707723Z",
     "shell.execute_reply": "2024-11-11T00:46:47.705581Z",
     "shell.execute_reply.started": "2024-11-11T00:45:13.285885Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded files with rows:\n",
      "\t1944210 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0/part-0.parquet\n",
      "\t2804247 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=1/part-0.parquet\n",
      "\t3036873 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=2/part-0.parquet\n",
      "\t4016784 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=3/part-0.parquet\n",
      "\t5022952 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=4/part-0.parquet\n",
      "\t5348200 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=5/part-0.parquet\n",
      "\t6203912 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=6/part-0.parquet\n",
      "\t6335560 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=7/part-0.parquet\n",
      "\t6140024 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=8/part-0.parquet\n",
      "\t6274576 : /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=9/part-0.parquet\n",
      "47127338 total samples.\n"
     ]
    }
   ],
   "source": [
    "def make_train_parquet_path(i: int) -> str:\n",
    "    return f\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id={i}/part-0.parquet\"\n",
    "# Setup the file indices to use.\n",
    "K_MAX_TRAIN_FILES: int = 10\n",
    "K_TRAIN_FILES: List[str] = [make_train_parquet_path(i) for i in range(K_MAX_TRAIN_FILES)]\n",
    "K_TEST_FILES: List[str] = [\"/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet/date_id=0/part-0.parquet\"]\n",
    "\n",
    "train_dataset = ParquetDataset(file_paths=K_TRAIN_FILES, logging=True)\n",
    "# test_dataset = ParquetDataset(file_paths=K_TEST_FILES, logging=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Run timing analysis to check batched approach vs. naive single-row loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:'iterate_samples_batched_row_group' args:[(), {'step_size': 1000}] took: 23.3475 sec\n",
      "func:'iterate_samples_batched_row_group' args:[(), {'step_size': 100}] took: 37.6389 sec\n"
     ]
    }
   ],
   "source": [
    "@timing\n",
    "def iterate_samples_batched_row_group(step_size: int):\n",
    "    for i in range(0, len(train_dataset), step_size):\n",
    "        train_dataset[i]\n",
    "\n",
    "iterate_samples_batched_row_group(step_size=1000)\n",
    "iterate_samples_batched_row_group(step_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a custom sample for randomly ordering the parquet files and rows within the parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParquetBatchedSampler(Sampler[int]):\n",
    "    r\"\"\"Samples elements randomly while batching by Parquet file to minimize disk i/o.\n",
    "    Randomly orders the N parquet files and then provides random indices into the rows of each parquet file,\n",
    "    ensuring that each file is fully sampled before movign on the to next.\n",
    "\n",
    "    Args:\n",
    "        data_source (Dataset): dataset to sample from\n",
    "        generator (Generator): Generator used in sampling.\n",
    "    \"\"\"\n",
    "\n",
    "    data_source: ParquetDataset\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_source: ParquetDataset,\n",
    "        generator=None,\n",
    "    ) -> None:\n",
    "        self.data_source = data_source\n",
    "        self.generator = generator\n",
    "\n",
    "    @property\n",
    "    def num_samples(self) -> int:\n",
    "        # dataset size might change at runtime\n",
    "        if self._num_samples is None:\n",
    "            return len(self.data_source)\n",
    "        return self._num_samples\n",
    "\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "\n",
    "        if self.generator is None:\n",
    "            seed = int(torch.empty((), dtype=torch.int64).random_().item())\n",
    "            generator = torch.Generator()\n",
    "            generator.manual_seed(seed)\n",
    "        else:\n",
    "            generator = self.generator\n",
    "\n",
    "        # Randomly sample from an ordering or partitions, shuffling all of the samples within a partition.\n",
    "        for i in torch.randperm(len(self.data_source.cum_total_counts), generator=generator):\n",
    "            file_idx, row_group_idx, offset = self.data_source.cum_total_counts[i]\n",
    "            yield from offset + torch.randperm(self.data_source.file_row_group_counts[file_idx][row_group_idx])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Verify that the sampler produces samples in random parition order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x74c508d55120>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bradley/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/bradley/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1568, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 47127338\n",
      "found new file 148\n",
      "found new file 140\n",
      "found new file 117\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[260], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sampler)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m new_idxes \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_index_from_cumulative_counts\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcum_total_counts_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m curr_partition \u001b[38;5;241m!=\u001b[39m new_idxes:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# update the partition index assuming that once we see a file index and then leave it\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# we'll never see it again. Throw an error if we do see it again.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_idxes \u001b[38;5;129;01min\u001b[39;00m partition_ordering:\n",
      "Cell \u001b[0;32mIn[257], line 82\u001b[0m, in \u001b[0;36mParquetDataset.calculate_index_from_cumulative_counts\u001b[0;34m(self, idx, cumulative_counts)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_index_from_cumulative_counts\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx: \u001b[38;5;28mint\u001b[39m, cumulative_counts: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearchsorted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcumulative_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mright\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:1534\u001b[0m, in \u001b[0;36msearchsorted\u001b[0;34m(a, v, side, sorter)\u001b[0m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_searchsorted_dispatcher)\n\u001b[1;32m   1465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearchsorted\u001b[39m(a, v, side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, sorter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1466\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1467\u001b[0m \u001b[38;5;124;03m    Find indices where elements should be inserted to maintain order.\u001b[39;00m\n\u001b[1;32m   1468\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1532\u001b[0m \n\u001b[1;32m   1533\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1534\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msearchsorted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mside\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:1149\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sampler = ParquetBatchedSampler(data_source=train_dataset)\n",
    "partition_ordering: List[Tuple[int,int]] = []\n",
    "num_sampled_per_partition: List[int] = []\n",
    "curr_partition: Optional[Tuple[int, int]] = None\n",
    "\n",
    "for i, idx in enumerate(sampler):\n",
    "\n",
    "    if (i % 1000000 == 0):\n",
    "        print(f\"{i} of {len(sampler)}\\r\")\n",
    "\n",
    "    new_idxes = train_dataset.calculate_index_from_cumulative_counts(idx, train_dataset.cum_total_counts_np)\n",
    "\n",
    "    if curr_partition != new_idxes:\n",
    "        # update the partition index assuming that once we see a file index and then leave it\n",
    "        # we'll never see it again. Throw an error if we do see it again.\n",
    "        if new_idxes in partition_ordering:\n",
    "            raise ValueError(f\"File index {new_idxes} seen out of order! Already seen ordering: {partition_ordering}\")\n",
    "        print(f\"found new file {new_idxes}\")\n",
    "        curr_partition = new_idxes\n",
    "        partition_ordering.append(curr_partition)\n",
    "        if len(num_sampled_per_partition) > 0:\n",
    "            num_sampled_per_partition.append(i - num_sampled_per_partition[-1])\n",
    "        else:\n",
    "            num_sampled_per_partition.append(i)\n",
    "\n",
    "print(f\"Total elements: {len(sampler)}. Sampled partition ordering: {partition_ordering}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Benchmarking how long it takes to serve samples from the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apparently there's an issue with \n",
    "@timing\n",
    "def iterate_dataloader(num_workers=0, batch_size=1, prefetch_factor=1):\n",
    "    dl = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        # prefetch_factor=prefetch_factor,\n",
    "        sampler=ParquetBatchedSampler(data_source=train_dataset)\n",
    "    )\n",
    "    print(\"Created dataloader.\")\n",
    "    for i, batch in enumerate(dl):\n",
    "        if i % 1000 == 0:\n",
    "            print(i, len(dl))\n",
    "\n",
    "iterate_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T21:17:16.113402Z",
     "iopub.status.busy": "2024-11-10T21:17:16.112741Z",
     "iopub.status.idle": "2024-11-10T21:17:19.029436Z",
     "shell.execute_reply": "2024-11-10T21:17:19.028057Z",
     "shell.execute_reply.started": "2024-11-10T21:17:16.113331Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "# Separate features and responders\n",
    "features = sample_df.filter(regex='^feature_')\n",
    "responders = sample_df.filter(regex='^responder_')\n",
    "# Convert to numpy arrays for TensorFlow\n",
    "X = features.values  # Features for input\n",
    "y = responders.values  # Responders for output\n",
    "X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "y = np.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T21:17:19.034077Z",
     "iopub.status.busy": "2024-11-10T21:17:19.033604Z",
     "iopub.status.idle": "2024-11-10T21:17:19.098752Z",
     "shell.execute_reply": "2024-11-10T21:17:19.097454Z",
     "shell.execute_reply.started": "2024-11-10T21:17:19.034029Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the number of input and output nodes\n",
    "input_dim = X.shape[1]  # Number of features (79)\n",
    "output_dim = y.shape[1]  # Number of responders (9)\n",
    "# Define the model\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(input_dim,)),  # Input layer\n",
    "    layers.Dense(64, activation='relu'),  # Encoder\n",
    "    layers.Dense(32, activation='relu'),  # Bottleneck layer (compression)\n",
    "    layers.Dense(64, activation='relu'),  # Decoder\n",
    "    layers.Dense(output_dim, activation='linear')  # Output layer for responders\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T21:17:19.100583Z",
     "iopub.status.busy": "2024-11-10T21:17:19.100159Z",
     "iopub.status.idle": "2024-11-10T21:17:19.106868Z",
     "shell.execute_reply": "2024-11-10T21:17:19.105404Z",
     "shell.execute_reply.started": "2024-11-10T21:17:19.100541Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "def step_decay(epoch):\n",
    "    initial_lr = 0.01\n",
    "    drop = 0.5\n",
    "    epochs_drop = 5\n",
    "    lr = initial_lr * (drop ** (epoch // epochs_drop))\n",
    "    return lr\n",
    "lr_scheduler = LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T21:17:19.109203Z",
     "iopub.status.busy": "2024-11-10T21:17:19.108678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Define EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',    # Monitor validation loss\n",
    "    patience=10,            # Number of epochs to wait for improvement\n",
    "    min_delta=0.001,       # Minimum change to qualify as an improvement\n",
    "    restore_best_weights=True  # Restore weights from the best epoch\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X, y,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save(\"/kaggle/working/model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "See [Jane Street RMF Demo Submission](https://www.kaggle.com/code/ryanholbrook/jane-street-rmf-demo-submission) for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import kaggle_evaluation.jane_street_inference_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "# Assuming `model` is your trained model\n",
    "# Assuming features required by the model are named 'feature_00', 'feature_01', etc.\n",
    "def predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Make a prediction.\"\"\"\n",
    "    global lags_\n",
    "    if lags is not None:\n",
    "        lags_ = lags\n",
    "    # Extract the features for the model input\n",
    "    feature_columns = [col for col in test.columns if col.startswith(\"feature_\")]\n",
    "    features = test.select(feature_columns).to_numpy()  # Convert to numpy array for model input\n",
    "    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    # Generate predictions using the model\n",
    "    model_predictions = model.predict(features)\n",
    "    responder_6_predictions = model_predictions[:, 6]  # Assuming responder_6 is at index 6\n",
    "    # Create a new Polars DataFrame with row_id and responder_6 predictions\n",
    "    predictions = test.select(\"row_id\").with_columns(\n",
    "        pl.Series(\"responder_6\", responder_6_predictions)\n",
    "    )\n",
    "    # Ensure the output format and length requirements\n",
    "    if isinstance(predictions, pl.DataFrame):\n",
    "        assert predictions.columns == ['row_id', 'responder_6']\n",
    "    elif isinstance(predictions, pd.DataFrame):\n",
    "        assert (predictions.columns == ['row_id', 'responder_6']).all()\n",
    "    else:\n",
    "        raise TypeError('The predict function must return a DataFrame')\n",
    "    \n",
    "    assert len(predictions) == len(test)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inference_server = kaggle_evaluation.jane_street_inference_server.JSInferenceServer(predict)\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "            '/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet',\n",
    "            '/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet',\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9871156,
     "sourceId": 84493,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
